{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(1) # A seed is set for reproducibility, ensuring that any random number generation will produce the same results each time the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relu(x):\n",
    "#     return (x > 0) * x # relu is a popular activation function used in neural networks. It returns the input if it's positive and returns 0 if it's negative or zero.\n",
    "\n",
    "# def relu_grad(x):\n",
    "#     return x > 0 # relu_grad returns the gradient (derivative) of the ReLU function. This gradient is 1 for positive inputs and 0 for negative or zero inputs.\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    sig_x = sigmoid(x)\n",
    "    return sig_x * (1 - sig_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetlights = np.array([[1,0,1], \n",
    "                         [0,1,1], \n",
    "                         [0,0,1], \n",
    "                         [1,1,1], \n",
    "                         [0,1,1], \n",
    "                         [1,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_vs_stop = np.array([[0], [1], [0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = streetlights, walk_vs_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nodes = 8 # specifies that the hidden layer of the network contains n nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100  # number of iterations to go through the network\n",
    "\n",
    "lr = 0.01      # how much we change the weights of the network each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5 #  weights for the input-to-hidden layers\n",
    "ws_2 = np.random.rand(hidden_nodes, y.shape[1]) - 0.5 # weights for the hidden-to-output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "0.118242\n",
      "0.195882\n",
      "0.217798\n",
      "0.219033\n",
      "0.213802\n",
      "0.20667\n",
      "0.199017\n",
      "0.191246\n",
      "0.183476\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):     #number of training iterations, or times to change the weights of the nn\n",
    "    for i in range(X.shape[0]): #for all samples in X, each streetlight\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        #forward pass/prediction\n",
    "        layer_1 = sigmoid(layer_in.dot(ws_1))\n",
    "        \n",
    "        layer_out = layer_1.dot(ws_2)\n",
    "        \n",
    "        #calc error/distance (how far are we from goal)\n",
    "        delta_2 = layer_out - y[i:i+1]\n",
    "        \n",
    "        #calc the the error each node in prev layer contributed\n",
    "        delta_1 = delta_2.dot(ws_2.T) * sigmoid_grad(layer_1)\n",
    "        \n",
    "        #update weights\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes,1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1],1).dot(delta_1))\n",
    "    \n",
    "    # Every 10 epochs, the squared error for the last sample is printed.\n",
    "    if epoch % 10 == 0:\n",
    "        error = delta_2**2\n",
    "        print(round(error[0][0],6))#, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results relu:\n",
    "0.019479\n",
    "0.046095\n",
    "0.062677\n",
    "0.061989\n",
    "0.052434\n",
    "0.041336\n",
    "0.031658\n",
    "0.024053\n",
    "0.018328\n",
    "0.01518\n",
    "\n",
    "Observations:\n",
    "Starts at 0.019479 and gradually decreases to 0.01518.\n",
    "Decreases in error are smaller with each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results sigmoid:\n",
    "1e-05\n",
    "0.118242\n",
    "0.195882\n",
    "0.217798\n",
    "0.219033\n",
    "0.213802\n",
    "0.20667\n",
    "0.199017\n",
    "0.191246\n",
    "0.183476\n",
    "\n",
    "Observations:\n",
    "Starts extremely low at 1e-05 (0.00001) and then increases substantially in the first two checkpoints.\n",
    "Peaks around 0.219033 and then begins to slightly decrease again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences did you experience?\n",
    "With ReLU, the initial error is much higher compared to the near-zero error of Sigmoid. However, the error in Sigmoid spikes significantly after the first iteration.\n",
    "\n",
    "The ReLU error consistently decreases, whereas the Sigmoid error goes up before starting to decrease again after reaching its peak.\n",
    "\n",
    "The maximum error in the ReLU case is much lower than that in the Sigmoid case.\n",
    "\n",
    "\n",
    "Why do you think this difference happened?\n",
    "ReLU and Sigmoid work differently. ReLU can produce a wide range of positive numbers, which might help it learn faster for this data. Sigmoid, on the other hand, gives numbers between 0 and 1, and this can cause some problems, like \"saturation\" or tiny changes that slow down learning. How we initially set up the system and the learning speed we pick also affects how well each function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001, Epochs taken: 29, Final error: 3.3451762178580132e-06\n",
      "Learning rate: 0.01, Epochs taken: 798, Final error: 0.0005182082504585931\n",
      "Learning rate: 0.1, Epochs taken: 87, Final error: 0.0002286416733087626\n",
      "Learning rate: 1, Epochs taken: 15, Final error: 0.002080598498202877\n",
      "Learning rate: 10, Epochs taken: 1, Final error: 9.02821384069402e-50\n",
      "\n",
      "Best Learning Rate: 10 with 1 epochs and error: 9.02821384069402e-50\n"
     ]
    }
   ],
   "source": [
    "### Code for testing different learning rates and epochs ###\n",
    "learning_rates = [0.001, 0.01, 0.1, 1, 10]\n",
    "max_epochs = 1000  # set a reasonable upper limit\n",
    "tolerance = 1e-5  # threshold to determine when error changes are insignificant\n",
    "\n",
    "best_lr = None\n",
    "best_epochs = None\n",
    "lowest_error = float('inf')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "    ws_2 = np.random.rand(hidden_nodes, y.shape[1]) - 0.5\n",
    "    \n",
    "    previous_error = float('inf')\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            layer_in = X[i:i+1]\n",
    "            layer_1 = sigmoid(layer_in.dot(ws_1))\n",
    "            layer_out = layer_1.dot(ws_2)\n",
    "            delta_2 = layer_out - y[i:i+1]\n",
    "            delta_1 = delta_2.dot(ws_2.T) * sigmoid_grad(layer_1)\n",
    "            \n",
    "            ws_2 -= lr * (layer_1.T.reshape(hidden_nodes,1).dot(delta_2))\n",
    "            ws_1 -= lr * (layer_in.T.reshape(X.shape[1],1).dot(delta_1))\n",
    "            \n",
    "        error = (delta_2**2).mean()\n",
    "        \n",
    "        # Check for plateau or increase in error\n",
    "        if abs(previous_error - error) < tolerance:\n",
    "            break\n",
    "        \n",
    "        previous_error = error\n",
    "        \n",
    "    print(f\"Learning rate: {lr}, Epochs taken: {epoch}, Final error: {error}\")\n",
    "    \n",
    "    if error < lowest_error:\n",
    "        best_lr = lr\n",
    "        best_epochs = epoch\n",
    "        lowest_error = error\n",
    "\n",
    "print(f\"\\nBest Learning Rate: {best_lr} with {best_epochs} epochs and error: {lowest_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing how fast our model learns, we see some trade-offs. Using a slow learning speed (like 0.001) gets us a pretty accurate result, but it takes a long time. On the other hand, speeding things up massively (with a rate like 10) makes the model learn a lot faster but might be too quick, possibly missing out on the best solution. When the error becomes really small, like with the rate of 10, the model might be just memorizing the data, rather than understanding its pattern, which isn't good. Meanwhile, a middle-speed rate, like 0.1, seems to be the best learning rate - not too slow, not too fast. However, the really fast learning rate of 10 worked surprisingly well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.520186\n",
      "0.426413\n",
      "0.420115\n",
      "0.413735\n",
      "0.407131\n",
      "0.400111\n",
      "0.39241\n",
      "0.383656\n",
      "0.373325\n",
      "0.360691\n"
     ]
    }
   ],
   "source": [
    "### adding another layer ###\n",
    "hidden_nodes = 8\n",
    "hidden_nodes_2 = 8  # second hidden layer with 8 nodes\n",
    "\n",
    "epochs = 100  # number of iterations to go through the network\n",
    "lr = 0.1      # how much we change the weights of the network each iteration\n",
    "\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # forward pass/prediction\n",
    "        layer_1 = sigmoid(layer_in.dot(ws_1))\n",
    "        layer_2 = sigmoid(layer_1.dot(ws_2))\n",
    "        layer_out = layer_2.dot(ws_3)\n",
    "        \n",
    "        # calc error/distance\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        \n",
    "        # propagate the error backward\n",
    "        delta_2 = delta_3.dot(ws_3.T) * sigmoid_grad(layer_2)\n",
    "        delta_1 = delta_2.dot(ws_2.T) * sigmoid_grad(layer_1)\n",
    "        \n",
    "        # update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        error = delta_3**2\n",
    "        print(round(error[0][0],6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these results with the single-layer network with sigmoid activation, the error values are significantly higher for the two-layer model. The single-layer network achieved errors as low as 1eâˆ’05 and mostly stayed below 0.22, while the two-layer network doesn't go below 0.360691 even after the same number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net (a):\n",
      "0.052689\n",
      "0.202071\n",
      "0.155539\n",
      "0.112984\n",
      "0.076007\n",
      "0.046419\n",
      "0.025315\n",
      "0.012132\n",
      "0.004979\n",
      "0.001645\n"
     ]
    }
   ],
   "source": [
    "### 4. Understanding the effect of activation function: Repeat the experiment as in 3. but by including an activation functions at various stages as shown. ###\n",
    "### neural net (a) ###\n",
    "hidden_nodes = 8\n",
    "hidden_nodes_2 = 6  # second hidden layer with 8 nodes\n",
    "\n",
    "epochs = 100  # number of iterations to go through the network\n",
    "lr = 0.1      # how much we change the weights of the network each iteration\n",
    "\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "print(\"Neural net (a):\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # forward pass/prediction\n",
    "        layer_1_linear = layer_in.dot(ws_1)\n",
    "        layer_1_active = sigmoid(layer_1_linear)\n",
    "        layer_2 = layer_1_active.dot(ws_2)\n",
    "        layer_out = layer_2.dot(ws_3)\n",
    "        \n",
    "        # calc error/distance\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        \n",
    "        # propagate the error backward\n",
    "        delta_2 = delta_3.dot(ws_3.T) * sigmoid_grad(layer_2)\n",
    "        delta_1 = delta_2.dot(ws_2.T) * sigmoid_grad(layer_1)\n",
    "        \n",
    "        # update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        error = delta_3**2\n",
    "        print(round(error[0][0],6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net (b):\n",
      "0.105736\n",
      "0.09129\n",
      "0.102012\n",
      "0.039496\n",
      "0.012884\n",
      "0.004266\n",
      "0.001618\n",
      "0.000755\n",
      "0.000442\n",
      "0.000312\n"
     ]
    }
   ],
   "source": [
    "### neural net (b) ###\n",
    "hidden_nodes = 8\n",
    "hidden_nodes_2 = 6  # second hidden layer with 8 nodes\n",
    "\n",
    "epochs = 100  # number of iterations to go through the network\n",
    "lr = 0.1      # how much we change the weights of the network each iteration\n",
    "\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "print(\"Neural net (b):\")\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # forward pass/prediction\n",
    "        layer_1 = layer_in.dot(ws_1)\n",
    "        layer_2_linear = layer_1.dot(ws_2)\n",
    "        layer_2_active = sigmoid(layer_2_linear)\n",
    "        layer_out = layer_2_active.dot(ws_3)\n",
    "        \n",
    "        # calc error/distance\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        \n",
    "        # propagate the error backward\n",
    "        delta_2 = delta_3.dot(ws_3.T) * sigmoid_grad(layer_2)\n",
    "        delta_1 = delta_2.dot(ws_2.T) * sigmoid_grad(layer_1)\n",
    "        \n",
    "        # update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        error = delta_3**2\n",
    "        print(round(error[0][0],6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net (c):\n",
      "0.248042\n",
      "0.360744\n",
      "0.360419\n",
      "0.360253\n",
      "0.360293\n",
      "0.360598\n",
      "0.361252\n",
      "0.362377\n",
      "0.364154\n",
      "0.36687\n"
     ]
    }
   ],
   "source": [
    "### neural net (c) ###\n",
    "hidden_nodes = 8\n",
    "hidden_nodes_2 = 6  # second hidden layer with 8 nodes\n",
    "\n",
    "epochs = 100  # number of iterations to go through the network\n",
    "lr = 0.1      # how much we change the weights of the network each iteration\n",
    "\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "print(\"Neural net (c):\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # forward pass/prediction\n",
    "        layer_1 = sigmoid(layer_in.dot(ws_1))\n",
    "        layer_2 = sigmoid(layer_1.dot(ws_2))\n",
    "        layer_out = layer_2.dot(ws_3)\n",
    "        layer_out = layer_2_active.dot(ws_3)\n",
    "        \n",
    "        # calc error/distance\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        \n",
    "        # propagate the error backward\n",
    "        delta_2 = delta_3.dot(ws_3.T) * sigmoid_grad(layer_2)\n",
    "        delta_1 = delta_2.dot(ws_2.T) * sigmoid_grad(layer_1)\n",
    "        \n",
    "        # update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        error = delta_3**2\n",
    "        print(round(error[0][0],6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net (a)\n",
    "Starts with an error of 0.052689, and by the end of training, it has reduced this error significantly to 0.001645. The decline in error shows that this configuration learns efficiently from the data.\n",
    "\n",
    "Neural net (b)\n",
    "It starts with an error of 0.105736 and drops to 0.000312 by the end. This implies that this model seems to be the most effective at reducing the error compared to the other two, given the significant drop.\n",
    "\n",
    "Neural net (c)\n",
    "Starts with an error of 0.248042, but the error only reduces slightly to 0.36687. The error seems to be reducing at a very slow pace compared to the other two."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
